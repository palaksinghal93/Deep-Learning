{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0iPOynczBfN+LKhDHulx3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8PXwFoX4jyoj","executionInfo":{"status":"ok","timestamp":1745395345493,"user_tz":-330,"elapsed":831,"user":{"displayName":"Palak Singhal","userId":"17497069916966824818"}},"outputId":"42297df4-f360-4459-dbbd-f0d365aadb98"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["a) NLTK Tokenization and Stemming:\n","['hello', '!', 'my', 'name', 'is', 'palak', 'singhal', '.', 'i', \"'m\", 'learn', 'natur', 'languag', 'process', '(', 'nlp', ')', 'use', 'python', '.', 'i', \"'ve\", 'never', 'studi', 'nlp', 'in', 'detail', 'and', 'now', 'i', \"'m\", 'love', 'thi', 'topic']\n","\n","b) NLTK Lemmatization and Stopword Removal:\n","['hello', '!', 'name', 'palak', 'singhal', '.', \"'m\", 'learning', 'natural', 'language', 'processing', '(', 'nlp', ')', 'using', 'python', '.', \"'ve\", 'never', 'studied', 'nlp', 'detail', \"'m\", 'loving', 'topic']\n","\n","c) spaCy Lemmatization and Stopword Removal:\n","['hello', 'Palak', 'Singhal', 'learn', 'Natural', 'Language', 'Processing', 'NLP', 'Python', 'study', 'NLP', 'detail', 'love', 'topic']\n"]}],"source":["# Install necessary libraries\n","# !pip install nltk spacy\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","import spacy\n","\n","# Download required resources for NLTK\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Input text\n","text = \"Hello! My name is Palak Singhal. I'm learning Natural Language Processing (NLP) using Python.\\n I've never studied NLP in detail and now I'm loving this topic\"\n","\n","# ---------------------------\n","# a) Tokenization and Stemming using NLTK\n","# ---------------------------\n","def nltk_tokenize_and_stem(text):\n","    stemmer = PorterStemmer()\n","    tokens = word_tokenize(text)\n","    stemmed = [stemmer.stem(token) for token in tokens]\n","    return stemmed\n","\n","# ---------------------------\n","# b) Lemmatization and Stopword Removal using NLTK\n","# ---------------------------\n","def nltk_lemmatize_and_remove_stopwords(text):\n","    lemmatizer = WordNetLemmatizer()\n","    stop_words = set(stopwords.words('english'))\n","    tokens = word_tokenize(text)\n","    filtered = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n","    return filtered\n","\n","# ---------------------------\n","# c) Lemmatization and Stopword Removal using spaCy\n","# ---------------------------\n","def spacy_lemmatize_and_remove_stopwords(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","    filtered = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n","    return filtered\n","\n","# Run all three functions\n","print(\"a) NLTK Tokenization and Stemming:\")\n","print(nltk_tokenize_and_stem(text))\n","\n","print(\"\\nb) NLTK Lemmatization and Stopword Removal:\")\n","print(nltk_lemmatize_and_remove_stopwords(text))\n","\n","print(\"\\nc) spaCy Lemmatization and Stopword Removal:\")\n","print(spacy_lemmatize_and_remove_stopwords(text))"]}]}