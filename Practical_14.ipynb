{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jdi6croilmL6"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"]},{"cell_type":"code","source":["# Sample text data\n","documents = [\n","    \"Deep learning is a subset of machine learning.\",\n","    \"Machine learning is used in data science.\",\n","    \"This is a Deep Learning Lab Manual.\"\n","]"],"metadata":{"id":"iP5zuhKWmLuV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a) Bag-of-Words using CountVectorizer\n","\n","def bow_with_countvectorizer(docs):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(docs)\n","    return vectorizer.get_feature_names_out(), X.toarray()\n"],"metadata":{"id":"31gYxpismLq1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# b) Bag-of-n-grams using CountVectorizer (bi-grams here)\n","\n","def ngrams_with_countvectorizer(docs, ngram_range=(2, 2)):\n","    vectorizer = CountVectorizer(ngram_range=ngram_range)\n","    X = vectorizer.fit_transform(docs)\n","    return vectorizer.get_feature_names_out(), X.toarray()\n","\n"],"metadata":{"id":"rkqXxTztmLno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# c) Bag-of-Words using TfidfVectorizer\n","\n","def bow_with_tfidfvectorizer(docs):\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(docs)\n","    return vectorizer.get_feature_names_out(), X.toarray()\n","\n"],"metadata":{"id":"wjSciAEqmYIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Running the methods\n","features_bow, matrix_bow = bow_with_countvectorizer(documents)\n","features_ngrams, matrix_ngrams = ngrams_with_countvectorizer(documents)\n","features_tfidf, matrix_tfidf = bow_with_tfidfvectorizer(documents)\n"],"metadata":{"id":"gFLL98AqmYEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display the results\n","print(\"a) Bag-of-Words (CountVectorizer):\")\n","print(\"Features:\", features_bow)\n","print(\"Matrix:\\n\", matrix_bow)\n","\n","print(\"\\n\\nb) Bag-of-n-grams (CountVectorizer - Bigrams):\")\n","print(\"Features:\", features_ngrams)\n","print(\"Matrix:\\n\", matrix_ngrams)\n","\n","print(\"\\n\\nc) Bag-of-Words (TfidfVectorizer):\")\n","print(\"Features:\", features_tfidf)\n","print(\"Matrix:\\n\", matrix_tfidf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YT0AgYPenTYf","outputId":"a75a16f3-bd0d-44bb-f514-66945b019e14","executionInfo":{"status":"ok","timestamp":1745394851014,"user_tz":-330,"elapsed":34,"user":{"displayName":"Palak Singhal","userId":"17497069916966824818"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a) Bag-of-Words (CountVectorizer):\n","Features: ['data' 'deep' 'in' 'is' 'lab' 'learning' 'machine' 'manual' 'of'\n"," 'science' 'subset' 'this' 'used']\n","Matrix:\n"," [[0 1 0 1 0 2 1 0 1 0 1 0 0]\n"," [1 0 1 1 0 1 1 0 0 1 0 0 1]\n"," [0 1 0 1 1 1 0 1 0 0 0 1 0]]\n","\n","\n","b) Bag-of-n-grams (CountVectorizer - Bigrams):\n","Features: ['data science' 'deep learning' 'in data' 'is deep' 'is subset' 'is used'\n"," 'lab manual' 'learning is' 'learning lab' 'machine learning' 'of machine'\n"," 'subset of' 'this is' 'used in']\n","Matrix:\n"," [[0 1 0 0 1 0 0 1 0 1 1 1 0 0]\n"," [1 0 1 0 0 1 0 1 0 1 0 0 0 1]\n"," [0 1 0 1 0 0 1 0 1 0 0 0 1 0]]\n","\n","\n","c) Bag-of-Words (TfidfVectorizer):\n","Features: ['data' 'deep' 'in' 'is' 'lab' 'learning' 'machine' 'manual' 'of'\n"," 'science' 'subset' 'this' 'used']\n","Matrix:\n"," [[0.         0.34353772 0.         0.26678769 0.         0.53357537\n","  0.34353772 0.         0.45171082 0.         0.45171082 0.\n","  0.        ]\n"," [0.43535684 0.         0.43535684 0.25712876 0.         0.25712876\n","  0.3311001  0.         0.         0.43535684 0.         0.\n","  0.43535684]\n"," [0.         0.36778358 0.         0.28561676 0.48359121 0.28561676\n","  0.         0.48359121 0.         0.         0.         0.48359121\n","  0.        ]]\n"]}]}]}